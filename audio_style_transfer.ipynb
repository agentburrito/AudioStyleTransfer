{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gram_matrix_max.ipynb","provenance":[{"file_id":"1SpfIcEjGHjLzVQhh5yrM1fNVmZu688iq","timestamp":1617521375011}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFkYE6YOyrOJ","executionInfo":{"status":"ok","timestamp":1618603634824,"user_tz":360,"elapsed":559,"user":{"displayName":"Max Melendez De La Cruz","photoUrl":"","userId":"16354415803231891827"}},"outputId":"4ca85c46-6a23-4ac1-a8ae-1e263b7f488e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1KhsLutgXc5lg4l323c49c-rSRxiz5QSv"},"id":"77MP4CUDhj9E","executionInfo":{"status":"error","timestamp":1618607613706,"user_tz":360,"elapsed":112942,"user":{"displayName":"Max Melendez De La Cruz","photoUrl":"","userId":"16354415803231891827"}},"outputId":"b5cf668d-adf3-4f6b-c88f-84bd4956c567"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import copy\n","import torch.optim as optim\n","import scipy\n","from scipy import signal\n","from scipy.io.wavfile import write\n","import matplotlib.pyplot as plt\n","import os\n","import PIL\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import numpy as np\n","import soundfile\n","import librosa\n","\n","def gram_matrix(map):\n","    batch_size, feature_maps, c = map.size()\n","    matrix = map.view(feature_maps * batch_size,c)\n","    gram = torch.mm(matrix,matrix.t())\n","    return gram.div(feature_maps * c)\n","\n","class StyleTransfer(nn.Module): #base class for all nn models\n","    def __init__(self,params):\n","        super(StyleTransfer,self).__init__() #call superclass constructor\n","        #load a pretrained vgg-19 model\n","        self.params = params\n","        self.model = nn.Sequential(nn.Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1),\n","                                   nn.ReLU())\n","    def get_model(self):\n","        print(self.model)\n","        return self.model.to(self.params[\"device\"])\n","    def forward(self,x):\n","        x = x.view(x.size(0),-1)\n","        return self.model(x)\n","\n","class StyleLoss(nn.Module):\n","    def __init__(self,target,weight):\n","        super(StyleLoss,self).__init__()\n","        self.weight = weight\n","        self.loss_target = gram_matrix(target.detach()) * weight\n","        self.loss_target = self.loss_target\n","    def forward(self,input):\n","        input_gram = gram_matrix(input.clone())\n","        input_gram.mul_(self.weight)\n","        # print(self.loss_target.device, input_gram.device)\n","        self.loss = F.mse_loss(self.loss_target,input_gram)\n","        return input\n","\n","class ContentLoss(nn.Module):\n","    def __init__(self,target):\n","        super(ContentLoss,self).__init__()\n","        self.loss_target = target.detach()\n","    def forward(self,input):\n","        self.loss = F.mse_loss(self.loss_target,input)\n","        return input\n","\n","\n","class Normalization(nn.Module):\n","    def __init__(self, mean, std):\n","        super(Normalization, self).__init__()\n","        # .view the mean and std to make them [C x 1 x 1] so that they can\n","        # directly work with image Tensor of shape [B x C x H x W].\n","        # B is batch size. C is number of channels. H is height and W is width.\n","        self.mean = torch.tensor(mean).view(-1, 1, 1)\n","        self.std = torch.tensor(std).view(-1, 1, 1)\n","\n","    def forward(self, img):\n","        # normalize img\n","        return (img - self.mean) / self.std\n","def setup_dataset():\n","    # jazz_loc = 'drive/MyDrive/jazz'\n","    #     class_loc = 'drive/MyDrive/class'\n","\n","    '''\n","    device: check if GPU has been added (runtime -> change runtime type), if not use a CPU,  \n","\n","    '''\n","    params = {\n","        'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","        'content_layers': ['conv_1'],\n","        'style_layers': ['conv_1'],\n","        's_contribution': [100],\n","        'c_contribution': [1],\n","        'lr': 0.01,\n","        'gamma': 0.01,\n","        's_weight': 1000000,\n","        'c_weight': 1,\n","        'epochs': 2001,\n","        'full_validate': 1000,\n","        'train_validate':50,\n","        'save_loc': 'drive/MyDrive/466 songs/max_save/',\n","        'model_save_loc': 'drive/MyDrive/466 songs/max_save/models/',\n","        'content_save_loc': 'drive/MyDrive/466 songs/max_samples/chainsaw.wav',\n","        'style_save_loc': 'drive/MyDrive/466 songs/max_samples/loopguitar.wav',\n","        'imsize': 128\n","    }\n","    # params['mean'] = torch.tensor([0.485, 0.456, 0.406]).to(params['device'])\n","    # params['std'] = torch.tensor([0.229, 0.224, 0.225]).to(params['device'])\n","    # def load_image(path):\n","    #     loader = transforms.Compose([transforms.Resize(params['imsize']),transforms.ToTensor()])\n","    #     img = PIL.Image.open(path)\n","    #     img = loader(img).unsqueeze(0)\n","    #     img = img[:,:3,:,:]\n","    #     print(2,img.shape)\n","    #     return img.to(params['device'],torch.float)\n","    # content = load_image(params['content_save_loc'])\n","    # style = load_image(params['style_save_loc'])\n","\n","    N_FFT=2048\n","    def read_audio_spectum(filename):\n","      x, fs = librosa.load(filename, duration=58.04) # Duration=58.05 so as to make sizes convenient\n","      S = librosa.stft(x, N_FFT)\n","      p = np.angle(S)\n","      S = np.log1p(np.abs(S))  \n","      return S, fs\n","\n","    style_audio, style_sr = read_audio_spectum(params['style_save_loc'])\n","    content_audio, content_sr = read_audio_spectum(params['content_save_loc'])\n","\n","    if(content_sr == style_sr):\n","      print('Sampling Rates are same')\n","    else:\n","      print('Sampling rates are not same')\n","      exit()\n","\n","    plt.figure()\n","    plt.imshow(style_audio)\n","    plt.axis('off')\n","    plt.savefig(\"style\" + \".png\",bbox_inches = 0)\n","    plt.ioff()\n","    plt.show()\n","\n","    plt.figure()\n","    plt.imshow(content_audio)\n","    plt.axis('off')\n","    plt.savefig(\"content_start\" + \".png\",bbox_inches = 0)\n","    plt.ioff()\n","    plt.show()       \n","\n","    style_audio = torch.from_numpy(style_audio)\n","    content_audio = torch.from_numpy(content_audio)\n","\n","    style = style_audio.unsqueeze(0)\n","    content = content_audio.unsqueeze(0)\n","    print(style.shape)\n","    return params,content, style, style_sr\n","\n","def setup_comp(cnn,params,style,content):\n","    #https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n","    style_vals = []\n","    content_vals = []\n","    s_done = False\n","    # normalization = Normalization(params['mean'], params['std']).to(params['device'])\n","    model = nn.Sequential()\n","\n","    print(style.shape)\n","    i = 0  # increment every time we see a conv'\n","    for layer in cnn.children():\n","        print(\"dd\")\n","        if isinstance(layer, nn.Conv1d) or isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = 'conv_{}'.format(i)\n","        model.add_module(name,layer)\n","\n","        if name in params['content_layers']:\n","            loc = params['content_layers'].index(name)\n","            if loc == len(params['content_layers']) - 1:\n","                c_done = True\n","            out = model(content).detach()\n","            content_loss = ContentLoss(out).to(params['device'])\n","            model.add_module(\"content_loss_{}\".format(i), content_loss)\n","            content_vals.append(content_loss)\n","\n","        if name in params['style_layers']:\n","            loc = params['style_layers'].index(name)\n","            if loc == len(params['style_layers']) - 1:\n","                s_done = True\n","            out = model(style).detach()\n","            style_loss = StyleLoss(out,params[\"s_weight\"]).to(params['device'])\n","            model.add_module(\"style_loss_{}\".format(i), style_loss)\n","            style_vals.append(style_loss)\n","        if s_done and c_done:\n","            break\n","    return model,style_vals, content_vals\n","\n","def apply(model,params,content,style_losses,content_losses, style_sr):\n","    content_in = nn.Parameter(content.data).to(params['device'])\n","    torch.cuda.empty_cache()\n","    # plt.figure()\n","    # plt.imshow(torch.squeeze(content).permute(1,2,0).detach().cpu())\n","    # plt.ioff()\n","    # plt.show()\n","    dir = os.listdir(params['model_save_loc'])\n","    dir = [item for item in dir if \".pt\" in item]\n","    dir.sort()\n","    print(dir)\n","    start = 1\n","    optimizer = optim.Adam([content_in.requires_grad_()],lr=params['lr'])\n","    # optimizer = optim.Adam(model.parameters(),lr = params['lr'], weight_decay = params['gamma'])\n","    if len(dir) != 0:\n","        newest = dir[len(dir) - 1]\n","        checkpoint = torch.load(params['model_save_loc'] + newest)\n","        start = checkpoint['Epoch'] + 1\n","        model.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['opt'])\n","        print(\"Loaded model, Epoch:\",start - 1,\"S_Loss:\",checkpoint['s_loss'])\n","    model.eval()\n","    for epoch in range(start,params['epochs']):\n","        optimizer.zero_grad()\n","        model(content_in)\n","        # print(out.shape)\n","        s_score = 0\n","        c_score = 0\n","        for loss, c in zip(style_losses,params['s_contribution']):\n","            s_score += loss.loss\n","        for loss, c in zip(content_losses,params['c_contribution']):\n","            c_score += loss.loss\n","        c_score = params['c_weight'] * c_score\n","        total_loss = s_score + c_score\n","        total_loss.backward()\n","        \n","        if epoch % params['train_validate'] == 0 or epoch == 1:\n","            save = torch.squeeze(content).detach().cpu()\n","            plt.figure()\n","            plt.imshow(save)\n","            plt.axis('off')\n","            plt.savefig(str(epoch) + \".png\",bbox_inches = 0)\n","            plt.ioff()\n","            plt.show()\n","\n","        if epoch % params['train_validate'] == 0:\n","            print('Epoch : {}, Total Loss : {:8f}, Style Loss : {:8f}, Content Loss : {:26f}'.format(epoch,total_loss.item(),s_score.item(),c_score.item()))\n","\n","        if epoch % params['full_validate'] == 0:\n","            N_FFT=2048\n","            output = content.squeeze()\n","            a = torch.exp(output) - 1\n","            a = a.detach().cpu().numpy()\n","            # This code is supposed to do phase reconstruction\n","            p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n","            for i in range(500):\n","                S = a * np.exp(1j*p)\n","                x = librosa.istft(S)\n","                p = np.angle(librosa.stft(x, N_FFT))\n","\n","            OUTPUT_FILENAME = params[\"save_loc\"] + str(epoch) + \"_chainsaw_loopguitar\" + '.wav'\n","\n","            soundfile.write(OUTPUT_FILENAME, x, style_sr,\"PCM_24\")\n","            # torch.save({\n","            #     'Epoch': epoch,\n","            #     'model': model.state_dict(),\n","            #     'opt': optimizer.state_dict(),\n","            #     's_loss': s_score.item(),\n","            # },params['model_save_loc'] + 'model.pt.' + str(epoch))\n","\n","        optimizer.step()\n","    return content\n","        \n","def main():\n","    params, content, style, style_sr = setup_dataset()\n","    model = StyleTransfer(params)\n","    model,styles,contents = setup_comp(model.get_model(),params,style.to(params['device']),content.to(params['device']))\n","    model = model.to(params['device'])\n","    content = content.to(params[\"device\"])\n","    apply(model,params,content.clone(),styles,contents,style_sr)\n","main()  \n"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}